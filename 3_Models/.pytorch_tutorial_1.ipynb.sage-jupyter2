{"backend_state":"init","kernel":"python3-ubuntu","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"57516e","input":"","pos":22,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"88a7f4","input":"","pos":56,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"b7ad41","input":"print(len(x_val))\nprint(len(y_val))","output":{"0":{"name":"stdout","output_type":"stream","text":"20\n20\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"44875e","input":"plt.figure(figsize=(10,5))\n\n# plot the train set \nplt.subplot(1,2,1)\nplt.scatter(x_train,y_train, c='orange')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Train Data')\nplt.grid('on')\n\n# plot the validation set \nplt.subplot(1,2,2)\nplt.scatter(x_val,x_val)  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Validation Data')\nplt.grid('on')\n\nplt.show()","output":{"0":{"data":{"image/png":"2adb379a86b2332a56b5fac154a3435185a4e326","text/plain":"<Figure size 720x360 with 2 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":342,"width":621},"needs_background":"light"},"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"a1da7c","input":"# initialize your random seed to ensure reproducibility of your result\nnp.random.seed(42)\n\n# Initializes parameters \"a\" and \"b\" randomly \na = np.random.randn(1)\nb = np.random.randn(1)\n\n# print values of a and b \nprint(a, b)\n","output":{"0":{"name":"stdout","output_type":"stream","text":"[0.49671415] [-0.1382643]\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"bcf87e","input":"# Initialization of hyper-parameters (in our case, only learning rate and number of epochs)\n\n# Sets learning rate\nlr = 1e-1\n# Defines number of epochs|\nn_epochs = 1000","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"db2cd3","input":"for epoch in range(n_epochs):\n    # Computes our model's predicted output\n    yhat = a + b * x_train\n    \n    # How wrong is our model? That's the error! \n    error = (y_train - yhat)\n    \n    # It is a regression, so it computes mean squared error (MSE)\n    loss = (error ** 2).mean()\n    \n    # Computes gradients for both \"a\" and \"b\" parameters\n    a_grad = -2 * error.mean()\n    b_grad = -2 * (x_train * error).mean()\n    \n    # Updates parameters using gradients and the learning rate\n    a = a - lr * a_grad  # new parameter a = old parameter a - learning rate * gradient of a\n    b = b - lr * b_grad\n    \nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"[1.02354094] [1.96896411]\n"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"a3d69d","input":"from sklearn.linear_model import LinearRegression\nlinr = LinearRegression()\nlinr.fit(x_train, y_train)\nprint(linr.intercept_, linr.coef_[0])","output":{"0":{"name":"stdout","output_type":"stream","text":"[1.02354075] [1.96896447]\n"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"a81966","input":"# import libraries \n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchviz import make_dot","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"b5e42c","input":"# Devices and CUDA\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"ac9651","input":"type(x_train)","output":{"0":{"data":{"text/plain":"numpy.ndarray"},"exec_count":18,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"609d49","input":"# load data \n\n# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n# and then we send them to the chosen device\nx_train_tensor = torch.from_numpy(x_train).float().to(device)\ny_train_tensor = torch.from_numpy(y_train).float().to(device)","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"9610b7","input":"# import libraries \nimport numpy as np \nimport matplotlib.pyplot as plt","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"e3eab6","input":"type(x_train_tensor)","output":{"0":{"data":{"text/plain":"torch.Tensor"},"exec_count":20,"output_type":"execute_result"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"9f3413","input":"# Here we can see the difference - notice that .type() is more useful\n# since it also tells us WHERE the tensor is (device)\nprint(type(x_train), type(x_train_tensor), x_train_tensor.type())","output":{"0":{"name":"stdout","output_type":"stream","text":"<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"152102","input":"# 1st method\n\n# FIRST\n# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n# since we want to apply gradient descent on these parameters, we need\n# to set REQUIRES_GRAD = TRUE\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float)\nb = torch.randn(1, requires_grad=True, dtype=torch.float)\nprint(a, b)\n\n# SECOND\n# But what if we want to run it on a GPU? We could just send them to device, right?\na = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)  # .to(device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\nprint(a, b)\n# Sorry, but NO! The to(device) \"shadows\" the gradient...","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\ntensor([0.2345], requires_grad=True) tensor([0.2303], requires_grad=True)\n"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"e1151b","input":"# 2nd method \n\n# FIRST\n# We can either create regular tensors and send them to the device (as we did with our data)\na = torch.randn(1, dtype=torch.float).to(device)\nb = torch.randn(1, dtype=torch.float).to(device)\n\n# SECOND\n# and THEN set them as requiring gradients...\na.requires_grad_()\nb.requires_grad_()\nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([-1.1229], requires_grad=True) tensor([-0.1863], requires_grad=True)\n"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"685336","input":"# 3rd method (it is much better to assign tensors to a device at the moment of their creation.)\n\n# We can specify the device at the moment of creation - RECOMMENDED!\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"4bad0f","input":"# Sets learning rate\nlr = 1e-1\n# Defines number of epochs|\nn_epochs = 1000","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"8817f0","input":"# Initializes parameters \"a\" and \"b\" randomly (using the 3rd method above)\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n\nprint(a,b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"249593","input":"for epoch in range(n_epochs):\n    \n    yhat = a + b * x_train_tensor  # prediction \n    error = y_train_tensor - yhat  # error = difference between (actual value and predicted value)\n    loss = (error ** 2).mean()     # loss (MSE)\n\n    # No more manual computation of gradients! \n    # a_grad = -2 * error.mean()\n    # b_grad = -2 * (x_tensor * error).mean()\n    \n    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n    loss.backward() # use the function .backward()\n#     # Let's check the computed gradients...\n#     print(a.grad)\n#     print(b.grad)\n    \n    # What about UPDATING the parameters? Not so fast...\n    \n    # FIRST METHOD\n    # AttributeError: 'NoneType' object has no attribute 'zero_'\n    # a = a - lr * a.grad\n    # b = b - lr * b.grad\n    # print(a)\n\n    # SECOND METHOD\n    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n    # a -= lr * a.grad\n    # b -= lr * b.grad        \n    \n    # THIRD METHOD\n    # We need to use NO_GRAD to keep the update out of the gradient computation\n    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n    with torch.no_grad():\n        a -= lr * a.grad\n        b -= lr * b.grad\n    \n    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n    a.grad.zero_()\n    b.grad.zero_()\n    \nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":28,"id":"4ce05b","input":"# let’s stick with two (gradient computing) tensors for our parameters, predictions, errors and loss.\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) # parameter a\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) # parameter b\n\nyhat = a + b * x_train_tensor  # prediction \nerror = y_train_tensor - yhat  # error\nloss = (error ** 2).mean()     # loss ","pos":41,"type":"cell"}
{"cell_type":"code","exec_count":29,"id":"087f8d","input":"make_dot(yhat)","output":{"0":{"data":{"image/svg+xml":"e4821f1bf61932a1e0c51a05386c476eb0061e8c","text/plain":"<graphviz.dot.Digraph at 0x7fd1b8ecb970>"},"exec_count":29,"output_type":"execute_result"}},"pos":42,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"a7f1f2","input":"# initialize your random seed to ensure reproducibility of your result\nnp.random.seed(42) # (The Story of Seed(42)) https://medium.com/@leticia.b/the-story-of-seed-42-874953452b94\n\n# randomly generate x which is avector of 100 points \nx = np.random.rand(100, 1)\n\n# define exact linear function y = 1 + 2x + epsilon where epsilon (0.1*random numbers)\n# 1 = y-intercept \n# 2 = slope \ny = 1 + 2 * x + .1 * np.random.randn(100, 1)","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"78f620","input":"make_dot(error)","output":{"0":{"data":{"image/svg+xml":"f98a81bbf617f3710cdd39c3359f971d77c2b83b","text/plain":"<graphviz.dot.Digraph at 0x7fd1b8ecbee0>"},"exec_count":30,"output_type":"execute_result"}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"c7e836","input":"make_dot(loss)","output":{"0":{"data":{"image/svg+xml":"9549260450eab275a9011d9cf776ab565bd67028","text/plain":"<graphviz.dot.Digraph at 0x7fd1b8ecb4c0>"},"exec_count":31,"output_type":"execute_result"}},"pos":45,"type":"cell"}
{"cell_type":"code","exec_count":32,"id":"360790","input":"torch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(a, b)\n","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"}},"pos":49,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"6e2ed2","input":"lr = 1e-1\nn_epochs = 1000\n\n# Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    loss.backward()    \n    \n    # No more manual update!\n    # with torch.no_grad():\n    #     a -= lr * a.grad\n    #     b -= lr * b.grad\n    optimizer.step()\n    \n    # No more telling PyTorch to let gradients go!\n    # a.grad.zero_()\n    # b.grad.zero_()\n    optimizer.zero_grad()\n    \nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"}},"pos":50,"type":"cell"}
{"cell_type":"code","exec_count":34,"id":"c534c7","input":"torch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":35,"id":"e75774","input":"lr = 1e-1\nn_epochs = 1000\n\n# Defines a MSE loss function\nloss_fn = nn.MSELoss(reduction='mean')\n\noptimizer = optim.SGD([a, b], lr=lr)\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    \n    # No more manual loss!\n    # error = y_tensor - yhat\n    # loss = (error ** 2).mean()\n    loss = loss_fn(y_train_tensor, yhat)\n\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"}},"pos":54,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"bfa654","input":"print(len(x))","output":{"0":{"name":"stdout","output_type":"stream","text":"100\n"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"be3619","input":"plt.figure(figsize=(10,5))\n\n# plot the entire data set \nplt.scatter(x,y, c='blue')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('entire dataset, function y = 1 + 2x + epsilon, a = 1, b = 2')\nplt.grid('on')","output":{"0":{"data":{"image/png":"bff62930748fcc95a9ec4eec7fecb27d7e5294a7","text/plain":"<Figure size 720x360 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":342,"width":619},"needs_background":"light"},"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"35167b","input":"idx = np.arange(100)\nprint(idx)","output":{"0":{"name":"stdout","output_type":"stream","text":"[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n 96 97 98 99]\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"dfd517","input":"# Shuffles the indices\nidx = np.arange(100)\nnp.random.shuffle(idx)\nprint(idx)","output":{"0":{"name":"stdout","output_type":"stream","text":"[76 83 80 98  2 77 71 84 89 50 40 51 67 86 37 49  4 10 69 81  9 54 55 87\n 64 44 90 75 33 30 93 95 14 61 11 13 15  7  0 19 35  6 12 65 70 88 56 58\n 28 38 91 42  8 73 39 85 25 92 41 26  1 22 21 46 74 79 78 72 57 53 24 17\n 66 32 31 62 59 52 82 23 36  5 45 99 43 16 48 94 34  3 18 47 60 68 63 27\n 96 29 20 97]\n"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"43b824","input":"# Uses first 80 random indices for train\ntrain_idx = idx[:80]\n\n# Uses the remaining indices for validation\nval_idx = idx[80:]\n\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\n","pos":9,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"2db997","input":"print(len(x_train))\nprint(len(y_train))","output":{"0":{"name":"stdout","output_type":"stream","text":"80\n80\n"}},"pos":10,"type":"cell"}
{"cell_type":"markdown","id":"0042eb","input":"## Loss","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"01ffdd","input":"## Optimizer","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"0e5c4e","input":"So far, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers, like SGD or Adam.\n\nAn optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.\n\nBesides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method and that’s it!\n\nIn the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters a and b.","pos":48,"type":"cell"}
{"cell_type":"markdown","id":"10c130","input":"## Dynamic Computation Graph\n\nThe PyTorchViz package and its make_dot(variable) method allows us to easily visualize a graph associated with a given Python variable.","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"13e9e2","input":"## Autograd\n\n\nAutograd is PyTorch’s automatic differentiation package. Thanks to it, we don’t need to worry about calculating the loss, gradient stuffs\n\nSo, how do we tell PyTorch to do its thing and compute all gradients? we use the function .backward()\n\nand how to update the parameters a and b? we use the function .zero_()","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"15b61f","input":"## Loading Data, Devices and CUDA","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"178c7e","input":"# stop here. we need to learn how to create python class/object first","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"320acf","input":"### For each epoch, there are 5 training steps:\n* Compute model’s predictions \n* Compute the error (the difference between the actual value and predicted value) \n* Compute the loss ( mean square error = the average of (error)^2)\n* Compute the gradients for every parameter (require calculus)\n* Update the parameters a and b","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"3e7f63","input":"## check our results use Scikit-learn's linear regression\n\nJust to make sure we haven’t done any mistakes in our code, we can use Scikit-Learn’s Linear Regression to fit the model and compare the coefficients.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"3f9d87","input":"## Creating Parameters","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"4bade3","input":"# Linear regression using Pytorch \n\nreview linear regression using NumPy from previous notebook ","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"4c33da","input":"## Data Generation","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"83d893","input":"## plot the train and validation sets","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"8ad9c2","input":"## linear regression using NumPy ","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"c22624","input":"# Linear regression using PyTorch","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"d59432","input":"### graphs for the prediction - yhat (left) the error (center) and loss (right) variables, \n\n* The only difference among them is the number of intermediate steps (gray boxes).\n\n* look at the green box of the left-most graph: there are two arrows pointing to it, since it is adding up two variables, a and b*x. Seems obvious, right?\n\n* look at the gray box of the same graph: it is performing a multiplication, namely, b*x. But there is only one arrow pointing to it! The arrow comes from the blue box that corresponds to our parameter b.\n\n* Why don’t we have a box for our data x? The answer is: we do not compute gradients for it! So, even though there are more tensors involved in the operations performed by the computation graph, it only shows gradient-computing tensors and its dependencies.\n\n\n<center />\n<img src=\"https://miro.medium.com/max/1400/1*K2QnR_TRF9XfqNgNGDRqng.png\" width=660 height=660 />\n\n","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"dc81d4","input":"## split data into train and validation sets (80/20)\n\nyou give 80% of the data points to the computer to learn and predict the line equation y = b0 + b1*x\n\nyou will use 20% of the dataset to test how good the algorithm is.","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"e33a75","input":"We now tackle the loss computation. As expected, PyTorch got us covered once again. There are many loss functions to choose from, depending on the task at hand. Since ours is a regression, we are using the Mean Square Error (MSE) loss.","pos":52,"type":"cell"}
{"cell_type":"markdown","id":"eb3bc7","input":"Let’s take a closer look at its components:\n\n* blue boxes: these correspond to the tensors we use as parameters, the ones we’re asking PyTorch to compute gradients for;\n\n* gray box: a Python operation that involves a gradient-computing tensor or its dependencies;\n\n* green box: the same as the gray box, except it is the starting point for the computation of gradients (assuming the backward()method is called from the variable used to visualize the graph)— they are computed from the bottom-up in a graph.","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"f3003e","input":"## The results!\n\nThey match up to 6 decimal places — we have a fully working implementation of linear regression using NumPy.","pos":21,"type":"cell"}
{"id":0,"time":1627541973076,"type":"user"}
{"last_load":1627541973254,"type":"file"}