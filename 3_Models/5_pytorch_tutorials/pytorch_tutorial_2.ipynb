{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Recall from previous notebook on linear regression using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# generate data \n",
    "\n",
    "# initialize your random seed to ensure reproducibility of your result\n",
    "np.random.seed(42) \n",
    "# randomly generate x which is avector of 100 points \n",
    "x = np.random.rand(100, 1)\n",
    "# define exact linear function y = 1 + 2x + epsilon where epsilon (0.1*random numbers)\n",
    "# 1 = y-intercept \n",
    "# 2 = slope \n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# split data into 80% train 20% validation \n",
    "\n",
    "idx = np.arange(100)\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Devices and CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7713],\n",
      "        [0.0636],\n",
      "        [0.8631],\n",
      "        [0.0254],\n",
      "        [0.7320],\n",
      "        [0.0740],\n",
      "        [0.1987],\n",
      "        [0.3110],\n",
      "        [0.4722],\n",
      "        [0.9696],\n",
      "        [0.1220],\n",
      "        [0.7751],\n",
      "        [0.8022],\n",
      "        [0.7296],\n",
      "        [0.0977],\n",
      "        [0.1849],\n",
      "        [0.1560],\n",
      "        [0.0206],\n",
      "        [0.9869],\n",
      "        [0.6233],\n",
      "        [0.7081],\n",
      "        [0.5979],\n",
      "        [0.9219],\n",
      "        [0.6376],\n",
      "        [0.2809],\n",
      "        [0.2588],\n",
      "        [0.1196],\n",
      "        [0.7290],\n",
      "        [0.9489],\n",
      "        [0.6075],\n",
      "        [0.5613],\n",
      "        [0.4938],\n",
      "        [0.1818],\n",
      "        [0.2713],\n",
      "        [0.9699],\n",
      "        [0.2123],\n",
      "        [0.1834],\n",
      "        [0.8662],\n",
      "        [0.3745],\n",
      "        [0.2912],\n",
      "        [0.8084],\n",
      "        [0.0581],\n",
      "        [0.8324],\n",
      "        [0.5427],\n",
      "        [0.7722],\n",
      "        [0.8872],\n",
      "        [0.0885],\n",
      "        [0.0452],\n",
      "        [0.5924],\n",
      "        [0.6842],\n",
      "        [0.7132],\n",
      "        [0.0344],\n",
      "        [0.6011],\n",
      "        [0.8155],\n",
      "        [0.4402],\n",
      "        [0.3252],\n",
      "        [0.7852],\n",
      "        [0.7608],\n",
      "        [0.4952],\n",
      "        [0.1997],\n",
      "        [0.9507],\n",
      "        [0.2921],\n",
      "        [0.1395],\n",
      "        [0.3117],\n",
      "        [0.7069],\n",
      "        [0.1159],\n",
      "        [0.3585],\n",
      "        [0.0055],\n",
      "        [0.1960],\n",
      "        [0.8948],\n",
      "        [0.4561],\n",
      "        [0.5248],\n",
      "        [0.1409],\n",
      "        [0.0651],\n",
      "        [0.1705],\n",
      "        [0.8287],\n",
      "        [0.3253],\n",
      "        [0.9395],\n",
      "        [0.3309],\n",
      "        [0.3664]])\n"
     ]
    }
   ],
   "source": [
    "# Convert Numpy arrays into PyTorch's Tensors, and then send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# initial guess a and b\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1  # learning rate\n",
    "n_epochs = 1000  # num of epochs\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor  # compute the predicted values yhat\n",
    "    loss = loss_fn(y_train_tensor, yhat)  # track the loss\n",
    "    loss.backward() # compute the gradient using auto_grad \n",
    "    optimizer.step() \n",
    "    optimizer.zero_grad() # update the parameters a and b\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "In PyTorch, a model is represented by a regular Python class that inherits from the Module class.\n",
    "\n",
    "The most fundamental methods are:\n",
    "    \n",
    "* ```__init__(self)```: it defines two parameters, a and b.\n",
    "\n",
    "* forward(self, x): it performs the actual computation, that is, it outputs a prediction, given the input x.\n",
    "\n",
    "\n",
    "You should NOT call the forward(x) method, though. You should call the whole model itself, as in model(x) to perform a forward pass and output predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### build a proper (yet simple) model for our regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the ```__init__``` method, we define our two parameters, a and b, using the Parameter() class, to tell PyTorch these tensors should be considered parameters of the model they are an attribute of.\n",
    "\n",
    "They used to be assigned as follows:\n",
    "\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### get the current values for all parameters using our model’s state_dict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lr = 1e-1  # learning rate\n",
    "n_epochs = 1000  # num of epochs\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # yhat = a + b * x_train_tensor  # compute the predicted values yhat\n",
    "    # What is this?!?\n",
    "    model.train()\n",
    "\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### comment “What is this?!?” — model.train().\n",
    "\n",
    "In PyTorch, models have a train() method which, somewhat disappointingly, does NOT perform a training step. Its only purpose is to set the model to training mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nested Models\n",
    "In our previous model, we manually created two parameters to perform a linear regression. That is: \n",
    "    \n",
    "self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "Let’s use PyTorch’s Linear model as an attribute of our own, thus creating a nested model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        # self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1) # 1 input feature (x value), 1 output feature (y value)\n",
    "                \n",
    "    def forward(self, x):\n",
    "\n",
    "        # return self.a + self.b * x\n",
    "        \n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sequential Models\n",
    "Our model was simple enougn. You may be thinking: “why even bother to build a class for it?!” Well, you have a point. \n",
    "    \n",
    "Since the output of a layer is sequentially fed as an input to the next, we can use a, er… Sequential model :-)\n",
    "    \n",
    "In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-0.4869]])), ('0.bias', tensor([0.5873]))])\n"
     ]
    }
   ],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)  # recurrent network \n",
    "losses = []\n",
    "\n",
    "# For each epoch... we train ... see how tiny the training loop is now?\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In PyTorch, a dataset is represented by a regular Python class that inherits from the Dataset class. \n",
    "\n",
    "\n",
    "The most fundamental methods are:\n",
    "\n",
    "* ```__init__(self)``` : it takes whatever arguments needed to build a list of tuples — it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n",
    "\n",
    "* ```__get_item__(self, index)```: it allows the dataset to be indexed, so it can work like a list (dataset[i]) — it must return a tuple (features, label) corresponding to the requested data point. We can either return the corresponding slices of our pre-loaded dataset or tensors or, as mentioned above, load them on demand (like in this example).\n",
    "\n",
    "* ```__len__(self)```: it should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let’s build a simple custom dataset that takes 2 tensors as arguments: \n",
    "\n",
    "* one for the features, \n",
    "* one for the labels. \n",
    "\n",
    "For any given index, our dataset class will return the corresponding slice of each of those tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 53,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 55,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Did you notice we built our training tensors out of Numpy arrays but we did not send them to a device? So, they are CPU tensors now! Why?\n",
    "\n",
    "We don’t want our whole training data to be loaded into GPU tensors, as we have been doing in our example so far, because it takes up space in our graphics card’s RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DataLoader\n",
    "Until now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small dataset (80 pair of points), sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. \n",
    "\n",
    "We use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!\n",
    "\n",
    "Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 57,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)  # total 80 points, divide into 10 bathes, each batch has 8 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1834],\n",
       "         [0.0452],\n",
       "         [0.8022],\n",
       "         [0.1395],\n",
       "         [0.7132],\n",
       "         [0.4722],\n",
       "         [0.0885],\n",
       "         [0.0055],\n",
       "         [0.6011],\n",
       "         [0.7320]]),\n",
       " tensor([[1.4637],\n",
       "         [0.9985],\n",
       "         [2.6229],\n",
       "         [1.3051],\n",
       "         [2.6162],\n",
       "         [1.9857],\n",
       "         [1.0708],\n",
       "         [1.0632],\n",
       "         [2.1214],\n",
       "         [2.4732]])]"
      ]
     },
     "execution_count": 58,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To retrieve a sample mini-batch, \n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "it will return a list containing two tensors, one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-0.4869]])), ('0.bias', tensor([0.5873]))])\n"
     ]
    }
   ],
   "source": [
    "# Let's put mini-batch into the training \n",
    "\n",
    "losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Two things are different now: \n",
    "\n",
    "* we have an inner loop to load each and every mini-batch from our DataLoader \n",
    "* we are now sending only one mini-batch to the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Random Split\n",
    "\n",
    "PyTorch’s random_split() method is an easy way of performing a training-validation split. Remeber that we need to apply it to the whole dataset (not the training dataset we built). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# convert numpy array to tensor CPU\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# load the data CPU\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# split the data into train and validation CPU\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# mini-batch for the train dataset\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "\n",
    "# mini-batch for the validation dataset\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluation (Last part YAY!)\n",
    "\n",
    "We need to change the training loop to include the evaluation of our model, that is, computing the validation loss. \n",
    "\n",
    "The first step is to include another inner loop to handle the mini-batches that come from the validation loader , sending them to the same device as our model. \n",
    "\n",
    "Next, we make predictions using our model and compute the corresponding loss.\n",
    "\n",
    "And there are TWO things need to consider:\n",
    "\n",
    "* torch.no_grad(): in the validation inner loop, we shall disable any gradient calculation;\n",
    "\n",
    "* eval(): the only thing it does is setting the model to evaluation mode (just like its train() counterpart did)\n",
    "\n",
    "here is our new training part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-0.4869]])), ('0.bias', tensor([0.5873]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer) # recurrent\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### the full program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Builds dataset with ALL data\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "# Splits randomly into train and validation datasets\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "# Builds a loader for each dataset to perform mini-batch gradient descent\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-0.9676]])), ('0.bias', tensor([-0.5727]))])\n"
     ]
    }
   ],
   "source": [
    "# Builds a simple sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Sets hyper-parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Defines loss function and optimizer\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "# Creates function to perform train step from model, loss and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        # One stpe of training\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "        \n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    with torch.no_grad(): # with statement will ensure that you never leave any resource open\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # What is that?!\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val)\n",
    "            # Computes validation loss\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9625]])), ('0.bias', tensor([1.0147]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028543626425166925\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008306218379487595\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the link for more details:\n",
    "\n",
    "https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3-ubuntu",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3-ubuntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}