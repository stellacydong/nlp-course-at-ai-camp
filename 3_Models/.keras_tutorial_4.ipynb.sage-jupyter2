{"backend_state":"init","kernel":"python3-ubuntu","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"25f958","input":"","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4b9cda","input":"","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6f5ebc","input":"","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9e9224","input":"","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"ead818","input":"import pandas as pd\n\nfilepath_dict = {'yelp':   'sentiment_data/yelp_labelled.txt',\n                 'amazon': 'sentiment_data/amazon_cells_labelled.txt',\n                 'imdb':   'sentiment_data/imdb_labelled.txt'}\n\ndf_list = []\nfor source, filepath in filepath_dict.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    df['source'] = source  # Add another column filled with the source name\n    df_list.append(df)\n\ndf = pd.concat(df_list)","pos":2,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"687a53","input":"# Run grid search for each source (yelp, amazon, imdb)\nfor source, frame in df.groupby('source'):\n    print('Running grid search for data set :', source)\n    sentences = df['sentence'].values\n    y = df['label'].values\n\n    # Train-test split\n    sentences_train, sentences_test, y_train, y_test = train_test_split(\n        sentences, y, test_size=0.25, random_state=1000)\n\n    # Tokenize words\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(sentences_train)\n    X_train = tokenizer.texts_to_sequences(sentences_train)\n    X_test = tokenizer.texts_to_sequences(sentences_test)\n\n    # Adding 1 because of reserved 0 index\n    vocab_size = len(tokenizer.word_index) + 1\n\n    # Pad sequences with zeros\n    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\n    # initialize the parameter grid for grid search\n    param_grid = dict(num_filters=[32, 64, 128],\n                      kernel_size=[3, 5, 7],\n                      vocab_size=[vocab_size],\n                      embedding_dim=[embedding_dim],\n                      maxlen=[maxlen])\n    \n    \n    model = KerasClassifier(build_fn=create_model,\n                            epochs=epochs, batch_size=10,\n                            verbose=False)\n    \n    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n                              cv=4, verbose=1, n_iter=5)\n\n    # By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. verbose=0 will show you nothing (silent), verbose=1 will show you an animated progress bar [====], verbose=2 will just mention the number of epoch like this: Epoch 1/5\n    \n    grid_result = grid.fit(X_train, y_train)\n\n    # Evaluate testing set\n    test_accuracy = grid.score(X_test, y_test)\n\n    # Save and evaluate results\n    prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n    if prompt.lower() not in {'y', 'true', 'yes'}:\n        break\n    with open(output_file, 'a') as f:\n        s = ('Running {} data set\\nBest Accuracy : '\n             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n        output_string = s.format(\n            source,\n            grid_result.best_score_,\n            grid_result.best_params_,\n            test_accuracy)\n        print(output_string)\n        f.write(output_string)","metadata":{"cocalc":{"outputs":{"1":{"name":"input","opts":{"password":false,"prompt":"finished amazon; write to file and proceed? [y/n]"},"output_type":"stream"},"4":{"name":"input","opts":{"password":false,"prompt":"finished imdb; write to file and proceed? [y/n]"},"output_type":"stream","value":"y"},"6":{"name":"input","opts":{"password":false,"prompt":"finished yelp; write to file and proceed? [y/n]"},"output_type":"stream","value":"y"}}}},"output":{"0":{"name":"stdout","output_type":"stream","text":"Running grid search for data set : amazon\nFitting 4 folds for each of 5 candidates, totalling 20 fits\n"},"1":{"name":"input","opts":{"password":false,"prompt":"finished amazon; write to file and proceed? [y/n]"},"output_type":"stream"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"617f36","input":"df.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                                            sentence  label source\n0                           Wow... Loved this place.      1   yelp\n1                                 Crust is not good.      0   yelp\n2          Not tasty and the texture was just nasty.      0   yelp\n3  Stopped by during the late May bank holiday of...      1   yelp\n4  The selection on the menu was great and so wer...      1   yelp"},"exec_count":2,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"e8d11b","input":"from sklearn.model_selection import train_test_split\n\ndf_yelp = df[df['source'] == 'yelp']\n\nsentences = df_yelp['sentence'].values\ny = df_yelp['label'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"b748f0","input":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)","pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"19fb27","input":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100  # for each sentence, the length of the vector = 100 \n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n","pos":9,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"87faee","input":"# define a function that creates a Keras model\n\ndef create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n    model = Sequential()\n    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(10, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"b3d299","input":"# initialize the parameter grid with the following dictionary:\n\n# param_grid = dict(num_filters=[32, 64, 128],\n#                   kernel_size=[3, 5, 7],\n#                   vocab_size=[5000], \n#                   embedding_dim=[50],\n#                   maxlen=[100])","pos":11,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"66a464","input":"from keras.models import Sequential\nfrom keras import layers\n######\nfrom keras.wrappers.scikit_learn import KerasClassifier  # cross-validation\nfrom sklearn.model_selection import RandomizedSearchCV   # grid search \n######","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"370b80","input":"# Main settings\nepochs = 20\nembedding_dim = 50\nmaxlen = 100\noutput_file = 'output.txt'","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"427038","input":"One method for hyperparameter optimization is **random search** which simply takes random combinations of parameters.\n\nYou will need two classes: \n* **KerasClassifier** which serves as a wrapper for the scikit-learn API. With this wrapper you are able to use the various tools available with scikit-learn like cross-validation. (Cross-validation is a way to validate the model and take the whole data set and separate it into multiple testing and training data sets.)\n\n* **RandomizedSearchCV** which implements random search with cross-validation. \n\nIn this example, we will show you the k-fold cross-validation (there are other types of cross-validation). In this type the data set is partitioned into k equal sized sets where one set is used for testing and the rest of the partitions are used for training. This enables you to run k different runs, where each partition is once used as a testing set. So, the higher k is the more accurate the model evaluation is, but the smaller each testing set is.\n\n\n<img src=\"https://www.mltut.com/wp-content/uploads/2020/05/cross-validation.png\" width=460 height=460 />\n\n\n\n\nFor k-folds cross-validation, please read the followings: \n\n    https://machinelearningmastery.com/k-fold-cross-validation/\n    https://www.mltut.com/k-fold-cross-validation-in-machine-learning-how-does-k-fold-work/","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"4ae1fb","input":"#### use padding to make sure each sentence have the same length vector representation ","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"5ad46d","input":"#### split data into training and testing ","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"9d0ac4","input":"### Use Keras to build CNN model \n##### Hyperparameters Optimization","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"c2229c","input":"#### Vectorization","pos":6,"type":"cell"}
{"id":0,"time":1627542465532,"type":"user"}
{"last_load":1627542465680,"type":"file"}