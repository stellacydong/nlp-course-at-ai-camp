{"backend_state":"init","kernel":"python3-ubuntu","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"686188","input":"","pos":55,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b44840","input":"","pos":58,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d46d26","input":"","pos":59,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e59724","input":"","pos":56,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f19ece","input":"","pos":57,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"1bb04e","input":"# import libraries\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchviz import make_dot","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":38,"id":"5cc8f0","input":"# initial guess a and b\ntorch.manual_seed(42)\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":39,"id":"b4d5ae","input":"lr = 1e-1  # learning rate\nn_epochs = 1000  # num of epochs\n\n# Defines a MSE loss function\nloss_fn = nn.MSELoss(reduction='mean')\n\n# Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor  # compute the predicted values yhat\n    loss = loss_fn(y_train_tensor, yhat)  # track the loss\n    loss.backward() # compute the gradient using auto_grad \n    optimizer.step() \n    optimizer.zero_grad() # update the parameters a and b\n    \nprint(a, b)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"45ef4d","input":"# generate data \n\n# initialize your random seed to ensure reproducibility of your result\nnp.random.seed(42) \n# randomly generate x which is avector of 100 points \nx = np.random.rand(100, 1)\n# define exact linear function y = 1 + 2x + epsilon where epsilon (0.1*random numbers)\n# 1 = y-intercept \n# 2 = slope \ny = 1 + 2 * x + .1 * np.random.randn(100, 1)","pos":2,"type":"cell"}
{"cell_type":"code","exec_count":40,"id":"8dfe4f","input":"class ManualLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        \n    def forward(self, x):\n        # Computes the outputs / predictions\n        return self.a + self.b * x","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":41,"id":"6aa329","input":"torch.manual_seed(42)\n# Now we can create a model and send it at once to the device\nmodel = ManualLinearRegression().to(device)\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":42,"id":"7f5828","input":"lr = 1e-1  # learning rate\nn_epochs = 1000  # num of epochs\n\n# Defines a MSE loss function\nloss_fn = nn.MSELoss(reduction='mean')\n\n# Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)","pos":14,"type":"cell"}
{"cell_type":"code","exec_count":43,"id":"9d8084","input":"for epoch in range(n_epochs):\n    \n    # yhat = a + b * x_train_tensor  # compute the predicted values yhat\n    # What is this?!?\n    model.train()\n\n    yhat = model(x_train_tensor)\n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":48,"id":"82fc09","input":"# Alternatively, you can use a Sequential model\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":49,"id":"621403","input":"def make_train_step(model, loss_fn, optimizer):\n    # Builds function that performs a step in the train loop\n    def train_step(x, y):\n        # Sets model to TRAIN mode\n        model.train()\n        # Makes predictions\n        yhat = model(x)\n        # Computes loss\n        loss = loss_fn(y, yhat)\n        # Computes gradients\n        loss.backward()\n        # Updates parameters and zeroes gradients\n        optimizer.step()\n        optimizer.zero_grad()\n        # Returns the loss\n        return loss.item()\n    \n    # Returns the function that will be called inside the train loop\n    return train_step\n\n# Creates the train_step function for our model, loss function and optimizer\ntrain_step = make_train_step(model, loss_fn, optimizer)  # recurrent network \nlosses = []\n\n# For each epoch... we train ... see how tiny the training loop is now?\nfor epoch in range(n_epochs):\n    # Performs one train step and returns the corresponding loss\n    loss = train_step(x_train_tensor, y_train_tensor)\n    losses.append(loss)\n    \n# Checks model's parameters\nprint(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('0.weight', tensor([[-0.4869]])), ('0.bias', tensor([0.5873]))])\n"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"085ef7","input":"# split data into 80% train 20% validation \n\nidx = np.arange(100)\n# Shuffles the indices\nidx = np.arange(100)\nnp.random.shuffle(idx)\n# Uses first 80 random indices for train\ntrain_idx = idx[:80]\n# Uses the remaining indices for validation\nval_idx = idx[80:]\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":50,"id":"00546e","input":"from torch.utils.data import Dataset, TensorDataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, x_tensor, y_tensor):\n        self.x = x_tensor\n        self.y = y_tensor\n        \n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.x)","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":51,"id":"2c588c","input":"# Wait, is this a CPU tensor now? Why? Where is .to(device)?\nx_train_tensor = torch.from_numpy(x_train).float()\ny_train_tensor = torch.from_numpy(y_train).float()","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":52,"id":"ddf425","input":"train_data = CustomDataset(x_train_tensor, y_train_tensor)\nprint(train_data[0])","output":{"0":{"name":"stdout","output_type":"stream","text":"(tensor([0.7713]), tensor([2.4745]))\n"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":53,"id":"85713b","input":"len(train_data)","output":{"0":{"data":{"text/plain":"80"},"exec_count":53,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":54,"id":"80ed0f","input":"train_data = TensorDataset(x_train_tensor, y_train_tensor)\nprint(train_data[0])","output":{"0":{"name":"stdout","output_type":"stream","text":"(tensor([0.7713]), tensor([2.4745]))\n"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":55,"id":"bc12f6","input":"len(train_data)","output":{"0":{"data":{"text/plain":"80"},"exec_count":55,"output_type":"execute_result"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":56,"id":"8586d4","input":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":57,"id":"c6261c","input":"len(train_loader)  # total 80 points, divide into 10 bathes, each batch has 8 points ","output":{"0":{"data":{"text/plain":"8"},"exec_count":57,"output_type":"execute_result"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":58,"id":"25d701","input":"# To retrieve a sample mini-batch, \nnext(iter(train_loader))","output":{"0":{"data":{"text/plain":"[tensor([[0.1834],\n         [0.0452],\n         [0.8022],\n         [0.1395],\n         [0.7132],\n         [0.4722],\n         [0.0885],\n         [0.0055],\n         [0.6011],\n         [0.7320]]),\n tensor([[1.4637],\n         [0.9985],\n         [2.6229],\n         [1.3051],\n         [2.6162],\n         [1.9857],\n         [1.0708],\n         [1.0632],\n         [2.1214],\n         [2.4732]])]"},"exec_count":58,"output_type":"execute_result"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":59,"id":"3a7533","input":"# Let's put mini-batch into the training \n\nlosses = []\ntrain_step = make_train_step(model, loss_fn, optimizer)\n\nfor epoch in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        # the dataset \"lives\" in the CPU, so do our mini-batches\n        # therefore, we need to send those mini-batches to the\n        # device where the model \"lives\"\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        loss = train_step(x_batch, y_batch)\n        losses.append(loss)\n        \nprint(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('0.weight', tensor([[-0.4869]])), ('0.bias', tensor([0.5873]))])\n"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"8ffd48","input":"# Devices and CUDA\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","pos":4,"type":"cell"}
{"cell_type":"code","exec_count":60,"id":"ea7bfe","input":"from torch.utils.data.dataset import random_split\n\n# convert numpy array to tensor CPU\nx_tensor = torch.from_numpy(x).float()\ny_tensor = torch.from_numpy(y).float()\n\n# load the data CPU\ndataset = TensorDataset(x_tensor, y_tensor)\n\n# split the data into train and validation CPU\ntrain_dataset, val_dataset = random_split(dataset, [80, 20])\n\n# mini-batch for the train dataset\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16)\n\n# mini-batch for the validation dataset\nval_loader = DataLoader(dataset=val_dataset, batch_size=20)","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":62,"id":"df2c33","input":"losses = []\nval_losses = []\n\ntrain_step = make_train_step(model, loss_fn, optimizer) # recurrent\n\nfor epoch in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        loss = train_step(x_batch, y_batch)\n        losses.append(loss)\n        \n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            x_val = x_val.to(device)\n            y_val = y_val.to(device)\n            \n            model.eval()\n\n            yhat = model(x_val)\n            val_loss = loss_fn(y_val, yhat)\n            val_losses.append(val_loss.item())\n\nprint(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('0.weight', tensor([[-0.4869]])), ('0.bias', tensor([0.5873]))])\n"}},"pos":42,"type":"cell"}
{"cell_type":"code","exec_count":64,"id":"63c207","input":"torch.manual_seed(42)\n\nx_tensor = torch.from_numpy(x).float()\ny_tensor = torch.from_numpy(y).float()\n","pos":44,"type":"cell"}
{"cell_type":"code","exec_count":65,"id":"f7cdc1","input":"# Builds dataset with ALL data\ndataset = TensorDataset(x_tensor, y_tensor)\n# Splits randomly into train and validation datasets\ntrain_dataset, val_dataset = random_split(dataset, [80, 20])\n# Builds a loader for each dataset to perform mini-batch gradient descent\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16)\nval_loader = DataLoader(dataset=val_dataset, batch_size=20)","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":66,"id":"5a4652","input":"# Builds a simple sequential model\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nprint(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('0.weight', tensor([[-0.9676]])), ('0.bias', tensor([-0.5727]))])\n"}},"pos":46,"type":"cell"}
{"cell_type":"code","exec_count":67,"id":"67839d","input":"# Sets hyper-parameters\nlr = 1e-1\nn_epochs = 150","pos":47,"type":"cell"}
{"cell_type":"code","exec_count":68,"id":"f09d2b","input":"# Defines loss function and optimizer\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":69,"id":"3b34f1","input":"losses = []\nval_losses = []\n# Creates function to perform train step from model, loss and optimizer\ntrain_step = make_train_step(model, loss_fn, optimizer)","pos":49,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"da57c5","input":"# Convert Numpy arrays into PyTorch's Tensors, and then send them to the chosen device\nx_train_tensor = torch.from_numpy(x_train).float().to(device)\ny_train_tensor = torch.from_numpy(y_train).float().to(device)","output":{"0":{"name":"stdout","output_type":"stream","text":"tensor([[0.7713],\n        [0.0636],\n        [0.8631],\n        [0.0254],\n        [0.7320],\n        [0.0740],\n        [0.1987],\n        [0.3110],\n        [0.4722],\n        [0.9696],\n        [0.1220],\n        [0.7751],\n        [0.8022],\n        [0.7296],\n        [0.0977],\n        [0.1849],\n        [0.1560],\n        [0.0206],\n        [0.9869],\n        [0.6233],\n        [0.7081],\n        [0.5979],\n        [0.9219],\n        [0.6376],\n        [0.2809],\n        [0.2588],\n        [0.1196],\n        [0.7290],\n        [0.9489],\n        [0.6075],\n        [0.5613],\n        [0.4938],\n        [0.1818],\n        [0.2713],\n        [0.9699],\n        [0.2123],\n        [0.1834],\n        [0.8662],\n        [0.3745],\n        [0.2912],\n        [0.8084],\n        [0.0581],\n        [0.8324],\n        [0.5427],\n        [0.7722],\n        [0.8872],\n        [0.0885],\n        [0.0452],\n        [0.5924],\n        [0.6842],\n        [0.7132],\n        [0.0344],\n        [0.6011],\n        [0.8155],\n        [0.4402],\n        [0.3252],\n        [0.7852],\n        [0.7608],\n        [0.4952],\n        [0.1997],\n        [0.9507],\n        [0.2921],\n        [0.1395],\n        [0.3117],\n        [0.7069],\n        [0.1159],\n        [0.3585],\n        [0.0055],\n        [0.1960],\n        [0.8948],\n        [0.4561],\n        [0.5248],\n        [0.1409],\n        [0.0651],\n        [0.1705],\n        [0.8287],\n        [0.3253],\n        [0.9395],\n        [0.3309],\n        [0.3664]])\n"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":71,"id":"60dbe8","input":"# Training loop\nfor epoch in range(n_epochs):\n    # Uses loader to fetch one mini-batch for training\n    for x_batch, y_batch in train_loader:\n        # NOW, sends the mini-batch data to the device\n        # so it matches location of the MODEL\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        # One stpe of training\n        loss = train_step(x_batch, y_batch)\n        losses.append(loss)\n        \n    # After finishing training steps for all mini-batches,\n    # it is time for evaluation!\n        \n    # We tell PyTorch to NOT use autograd...\n    with torch.no_grad(): # with statement will ensure that you never leave any resource open\n        # Uses loader to fetch one mini-batch for validation\n        for x_val, y_val in val_loader:\n            # Again, sends data to same device as model\n            x_val = x_val.to(device)\n            y_val = y_val.to(device)\n            \n            # What is that?!\n            model.eval()\n            # Makes predictions\n            yhat = model(x_val)\n            # Computes validation loss\n            val_loss = loss_fn(y_val, yhat)\n            val_losses.append(val_loss.item())\n","pos":50,"type":"cell"}
{"cell_type":"code","exec_count":72,"id":"0ab667","input":"print(model.state_dict())","output":{"0":{"name":"stdout","output_type":"stream","text":"OrderedDict([('0.weight', tensor([[1.9625]])), ('0.bias', tensor([1.0147]))])\n"}},"pos":51,"type":"cell"}
{"cell_type":"code","exec_count":73,"id":"534029","input":"print(np.mean(losses))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.028543626425166925\n"}},"pos":52,"type":"cell"}
{"cell_type":"code","exec_count":74,"id":"7a4444","input":"print(np.mean(val_losses))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.008306218379487595\n"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":75,"id":"7c1f04","input":"class LayerLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        # self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        # Instead of our custom parameters, we use a Linear layer with single input and single output\n        self.linear = nn.Linear(1, 1) # 1 input feature (x value), 1 output feature (y value)\n                \n    def forward(self, x):\n\n        # return self.a + self.b * x\n        \n        # Now it only takes a call to the layer to make predictions\n        return self.linear(x)","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"0520f9","input":"### get the current values for all parameters using our model’s state_dict() method.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"368f7f","input":"### DataLoader\nUntil now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small dataset (80 pair of points), sure, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. \n\nWe use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!\n\nOur loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"44ecbd","input":"it will return a list containing two tensors, one for the features, another one for the labels.","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"5a366a","input":"Did you notice we built our training tensors out of Numpy arrays but we did not send them to a device? So, they are CPU tensors now! Why?\n\nWe don’t want our whole training data to be loaded into GPU tensors, as we have been doing in our example so far, because it takes up space in our graphics card’s RAM.","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"5ec097","input":"In the ```__init__``` method, we define our two parameters, a and b, using the Parameter() class, to tell PyTorch these tensors should be considered parameters of the model they are an attribute of.\n\nThey used to be assigned as follows:\n\na = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"6166bd","input":"### Random Split\n\nPyTorch’s random_split() method is an easy way of performing a training-validation split. Remeber that we need to apply it to the whole dataset (not the training dataset we built). ","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"626414","input":"### build a proper (yet simple) model for our regression task.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"72c982","input":"## Recall from previous notebook on linear regression using PyTorch","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"7743ad","input":"## Nested Models\nIn our previous model, we manually created two parameters to perform a linear regression. That is: \n    \nself.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n\nself.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n        \nLet’s use PyTorch’s Linear model as an attribute of our own, thus creating a nested model.\n","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"7a56ab","input":"Let’s build a simple custom dataset that takes 2 tensors as arguments: \n\n* one for the features, \n* one for the labels. \n\nFor any given index, our dataset class will return the corresponding slice of each of those tensors. ","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"7b1d79","input":"### Dataset\n\nIn PyTorch, a dataset is represented by a regular Python class that inherits from the Dataset class. \n\n\nThe most fundamental methods are:\n\n* ```__init__(self)``` : it takes whatever arguments needed to build a list of tuples — it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n\n* ```__get_item__(self, index)```: it allows the dataset to be indexed, so it can work like a list (dataset[i]) — it must return a tuple (features, label) corresponding to the requested data point. We can either return the corresponding slices of our pre-loaded dataset or tensors or, as mentioned above, load them on demand (like in this example).\n\n* ```__len__(self)```: it should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"8c07a7","input":"### Evaluation (Last part YAY!)\n\nWe need to change the training loop to include the evaluation of our model, that is, computing the validation loss. \n\nThe first step is to include another inner loop to handle the mini-batches that come from the validation loader , sending them to the same device as our model. \n\nNext, we make predictions using our model and compute the corresponding loss.\n\nAnd there are TWO things need to consider:\n\n* torch.no_grad(): in the validation inner loop, we shall disable any gradient calculation;\n\n* eval(): the only thing it does is setting the model to evaluation mode (just like its train() counterpart did)\n\nhere is our new training part ","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"8d43ac","input":"#### Training Step","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"949049","input":"### Check the link for more details:\n\nhttps://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e","pos":54,"type":"cell"}
{"cell_type":"markdown","id":"a306d5","input":"#### comment “What is this?!?” — model.train().\n\nIn PyTorch, models have a train() method which, somewhat disappointingly, does NOT perform a training step. Its only purpose is to set the model to training mode. ","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"a513e7","input":"## Model\nIn PyTorch, a model is represented by a regular Python class that inherits from the Module class.\n\nThe most fundamental methods are:\n    \n* ```__init__(self)```: it defines two parameters, a and b.\n\n* forward(self, x): it performs the actual computation, that is, it outputs a prediction, given the input x.\n\n\nYou should NOT call the forward(x) method, though. You should call the whole model itself, as in model(x) to perform a forward pass and output predictions.\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"e57a2d","input":"## Sequential Models\nOur model was simple enougn. You may be thinking: “why even bother to build a class for it?!” Well, you have a point. \n    \nSince the output of a layer is sequentially fed as an input to the next, we can use a, er… Sequential model :-)\n    \nIn our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. ","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"f21f3f","input":"### the full program","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"f38425","input":"Two things are different now: \n\n* we have an inner loop to load each and every mini-batch from our DataLoader \n* we are now sending only one mini-batch to the device.","pos":38,"type":"cell"}
{"id":0,"time":1627541968906,"type":"user"}
{"last_load":1627541969002,"type":"file"}