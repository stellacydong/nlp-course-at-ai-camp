{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use Keras to build CNN model \n",
    "##### Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One method for hyperparameter optimization is **random search** which simply takes random combinations of parameters.\n",
    "\n",
    "You will need two classes: \n",
    "* **KerasClassifier** which serves as a wrapper for the scikit-learn API. With this wrapper you are able to use the various tools available with scikit-learn like cross-validation. (Cross-validation is a way to validate the model and take the whole data set and separate it into multiple testing and training data sets.)\n",
    "\n",
    "* **RandomizedSearchCV** which implements random search with cross-validation. \n",
    "\n",
    "In this example, we will show you the k-fold cross-validation (there are other types of cross-validation). In this type the data set is partitioned into k equal sized sets where one set is used for testing and the rest of the partitions are used for training. This enables you to run k different runs, where each partition is once used as a testing set. So, the higher k is the more accurate the model evaluation is, but the smaller each testing set is.\n",
    "\n",
    "\n",
    "<img src=\"https://www.mltut.com/wp-content/uploads/2020/05/cross-validation.png\" width=460 height=460 />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For k-folds cross-validation, please read the followings: \n",
    "\n",
    "    https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "    https://www.mltut.com/k-fold-cross-validation-in-machine-learning-how-does-k-fold-work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath_dict = {'yelp':   'sentiment_data/yelp_labelled.txt',\n",
    "                 'amazon': 'sentiment_data/amazon_cells_labelled.txt',\n",
    "                 'imdb':   'sentiment_data/imdb_labelled.txt'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label source\n",
       "0                           Wow... Loved this place.      1   yelp\n",
       "1                                 Crust is not good.      0   yelp\n",
       "2          Not tasty and the texture was just nasty.      0   yelp\n",
       "3  Stopped by during the late May bank holiday of...      1   yelp\n",
       "4  The selection on the menu was great and so wer...      1   yelp"
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### split data into training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = df_yelp['sentence'].values\n",
    "y = df_yelp['label'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### use padding to make sure each sentence have the same length vector representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100  # for each sentence, the length of the vector = 100 \n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# define a function that creates a Keras model\n",
    "\n",
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# initialize the parameter grid with the following dictionary:\n",
    "\n",
    "# param_grid = dict(num_filters=[32, 64, 128],\n",
    "#                   kernel_size=[3, 5, 7],\n",
    "#                   vocab_size=[5000], \n",
    "#                   embedding_dim=[50],\n",
    "#                   maxlen=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "######\n",
    "from keras.wrappers.scikit_learn import KerasClassifier  # cross-validation\n",
    "from sklearn.model_selection import RandomizedSearchCV   # grid search \n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Main settings\n",
    "epochs = 20\n",
    "embedding_dim = 50\n",
    "maxlen = 100\n",
    "output_file = 'output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cocalc": {
     "outputs": {
      "1": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": "finished amazon; write to file and proceed? [y/n]"
       },
       "output_type": "stream"
      },
      "4": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": "finished imdb; write to file and proceed? [y/n]"
       },
       "output_type": "stream",
       "value": "y"
      },
      "6": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": "finished yelp; write to file and proceed? [y/n]"
       },
       "output_type": "stream",
       "value": "y"
      }
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search for data set : amazon\n",
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "finished amazon; write to file and proceed? [y/n] "
    }
   ],
   "source": [
    "# Run grid search for each source (yelp, amazon, imdb)\n",
    "for source, frame in df.groupby('source'):\n",
    "    print('Running grid search for data set :', source)\n",
    "    sentences = df['sentence'].values\n",
    "    y = df['label'].values\n",
    "\n",
    "    # Train-test split\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "    # Adding 1 because of reserved 0 index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Pad sequences with zeros\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "    # initialize the parameter grid for grid search\n",
    "    param_grid = dict(num_filters=[32, 64, 128],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen])\n",
    "    \n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, batch_size=10,\n",
    "                            verbose=False)\n",
    "    \n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=1, n_iter=5)\n",
    "\n",
    "    # By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. verbose=0 will show you nothing (silent), verbose=1 will show you an animated progress bar [====], verbose=2 will just mention the number of epoch like this: Epoch 1/5\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate testing set\n",
    "    test_accuracy = grid.score(X_test, y_test)\n",
    "\n",
    "    # Save and evaluate results\n",
    "    prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
    "    if prompt.lower() not in {'y', 'true', 'yes'}:\n",
    "        break\n",
    "    with open(output_file, 'a') as f:\n",
    "        s = ('Running {} data set\\nBest Accuracy : '\n",
    "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
    "        output_string = s.format(\n",
    "            source,\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_,\n",
    "            test_accuracy)\n",
    "        print(output_string)\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3-ubuntu",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3-ubuntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}