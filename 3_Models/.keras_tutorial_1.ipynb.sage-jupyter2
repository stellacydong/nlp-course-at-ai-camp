{"backend_state":"init","kernel":"python3-ubuntu","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"0b99c9","input":"","pos":28,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b23c9e","input":"","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d9663b","input":"","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e5e8b6","input":"","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e7aee6","input":"","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"f0ce79","input":"import pandas as pd\n\nfilepath_dict = {'yelp':   'sentiment_data/yelp_labelled.txt',\n                 'amazon': 'sentiment_data/amazon_cells_labelled.txt',\n                 'imdb':   'sentiment_data/imdb_labelled.txt'}\n\ndf_list = []\nfor source, filepath in filepath_dict.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    df['source'] = source  # Add another column filled with the source name\n    df_list.append(df)\n\ndf = pd.concat(df_list)","pos":4,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"27ed66","input":"model.summary()","output":{"0":{"name":"stdout","output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                17150     \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 17,161\nTrainable params: 17,161\nNon-trainable params: 0\n_________________________________________________________________\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"3ddb9f","input":"history = model.fit(X_train, y_train, \n                    epochs=100,                   # 100 times of training \n                    verbose=False,\n                    validation_data=(X_test, y_test), \n                    batch_size=10)               # for each training, how many dataset you want to use \n","output":{"0":{"name":"stderr","output_type":"stream","text":"/projects/0ca79bd3-29e8-484c-b720-822c99947c31/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  warnings.warn(\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"1111cc","input":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","output":{"0":{"name":"stdout","output_type":"stream","text":"Training Accuracy: 1.0000\nTesting Accuracy:  0.7920\n"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"92e35e","input":"history.history.keys()","output":{"0":{"data":{"text/plain":"dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"},"exec_count":13,"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"d7d659","input":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"8f4683","input":"plot_history(history)","output":{"0":{"data":{"image/png":"52a5c0ee62225f8d796c3bb473ae15d5d5d330e7","text/plain":"<Figure size 864x360 with 2 Axes>"},"exec_count":15,"metadata":{"image/png":{"height":319,"width":706}},"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"7667e7","input":"df.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                                            sentence  label source\n0                           Wow... Loved this place.      1   yelp\n1                                 Crust is not good.      0   yelp\n2          Not tasty and the texture was just nasty.      0   yelp\n3  Stopped by during the late May bank holiday of...      1   yelp\n4  The selection on the menu was great and so wer...      1   yelp"},"exec_count":2,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"58dd30","input":"from sklearn.model_selection import train_test_split\n\ndf_yelp = df[df['source'] == 'yelp']\n\nsentences = df_yelp['sentence'].values\ny = df_yelp['label'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)","pos":7,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"9cd9ca","input":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)\nX_train","output":{"0":{"data":{"text/plain":"<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n\twith 7368 stored elements in Compressed Sparse Row format>"},"exec_count":4,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"04561e","input":"from keras.models import Sequential\nfrom keras import layers","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"dedbc3","input":"input_dim = X_train.shape[1]  # Number of features","pos":13,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"cbc7dd","input":"X_train.shape","output":{"0":{"data":{"text/plain":"(750, 1714)"},"exec_count":7,"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"5e449b","input":"input_dim","output":{"0":{"data":{"text/plain":"1714"},"exec_count":8,"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"df5d54","input":"# Dense layer == similar to perpectron hidden layer which has weights, bias \n\nmodel = Sequential()\n# output = 10, input = size of vocabulary, activation function = relu \nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu')) \n\n# output = 1, activation function = sigmoid \nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"0afcdc","input":"### deep learning with keras ","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"370d49","input":"#### use the CountVectorizer provided by the scikit-learn library to vectorize sentences","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"3aaf86","input":"#### Keras supports two main types of models:\n    \n* the Sequential model API which you are going to see in use in this tutorial, and \n* the functional API which can do everything of the Sequential model but it can be also used for advanced models with complex network architectures.\n\nThe Sequential model is a linear stack of layers, where you can use the large variety of available layers in Keras. The most common layer is the Dense layer which is your regular densely connected neural network layer with all the weights and biases that you are already familiar with.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"3ad951","input":"#### load data \n\nhttps://archive-beta.ics.uci.edu/ml/datasets/331","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"51c3da","input":"#### Introducing Keras\nKeras is a deep learning and neural networks API by François Chollet which is capable of running on top of Tensorflow (Google), Theano or CNTK (Microsoft).","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"658368","input":"You can see that we have trained our model for too long since the training set reached 100% accuracy. A good way to see when the model starts overfitting is when the loss of the validation data starts rising again. This tends to be a good point to stop the model. You can see this around 20-40 epochs in this training.","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"847b00","input":"#### split training and testing dataset.","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"bbfb5f","input":"You can see that the resulting feature vectors have 750 samples which are the number of training samples we have after the train-test split. Each sample has 2505 dimensions which is the size of the vocabulary. ","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"c8b99f","input":"To quote the wonderful book by François Chollet, Deep Learning with Python:\n\n>Keras is a model-level library, providing high-level building blocks for developing deep-learning models. It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the backend engine of Keras\n\nIt is a great way to start experimenting with neural networks without having to implement every layer and piece on your own. For example Tensorflow is a great machine learning library, but you have to implement a lot of boilerplate code to have a model running.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"f280f9","input":"You can already see that the model was overfitting since it reached 100% accuracy for the training set. But this was expected since the number of epochs was fairly large for this model. However, the accuracy of the testing set has already surpassed our previous logistic Regression with BOW model, which is a great step further in terms of our progress.","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"f692bb","input":"You might notice that we have 17150 parameters for the first layer and another 11 in the second one. \n\nWhere did those come from?\n\nWe have 1714 dimensions for each feature vector, and then we have 10 nodes. We need weights for each feature dimension and each node which accounts for 1714 * 10 = 17140 parameters, and then we have another 10 times an added bias for each node, which gets us the 17140+10 = 17150 parameters. \n\nIn the final node, we have another 10 weights and one bias, which gets us to 11 parameters. \n\nThat’s a total of 17150 + 11 = 17161  parameters for both layers.","pos":18,"type":"cell"}
{"id":0,"time":1627542365271,"type":"user"}
{"last_load":1627542365373,"type":"file"}