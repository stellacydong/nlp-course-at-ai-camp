{"backend_state":"running","connection_file":"/projects/ae15c660-30de-474e-abca-5963358c9eb9/.local/share/jupyter/runtime/kernel-982174c7-7aab-4e6d-811d-9702ca45e637.json","kernel":"python3-ubuntu","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1627542263161,"exec_count":1,"id":"c010e6","input":"import pandas as pd\n\nfilepath_dict = {'yelp':   'sentiment_data/yelp_labelled.txt',\n                 'amazon': 'sentiment_data/amazon_cells_labelled.txt',\n                 'imdb':   'sentiment_data/imdb_labelled.txt'}\n\ndf_list = []\nfor source, filepath in filepath_dict.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    df['source'] = source  # Add another column filled with the source name\n    df_list.append(df)\n\ndf = pd.concat(df_list)","kernel":"python3-ubuntu","pos":1,"start":1627542262982,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"30376f","input":"","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4ecbcc","input":"","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ef02f7","input":"","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f5f088","input":"","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"a69811","input":"df.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n      <td>yelp</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                                            sentence  label source\n0                           Wow... Loved this place.      1   yelp\n1                                 Crust is not good.      0   yelp\n2          Not tasty and the texture was just nasty.      0   yelp\n3  Stopped by during the late May bank holiday of...      1   yelp\n4  The selection on the menu was great and so wer...      1   yelp"},"exec_count":2,"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"cf9987","input":"from sklearn.model_selection import train_test_split\n\ndf_yelp = df[df['source'] == 'yelp']\n\nsentences = df_yelp['sentence'].values\ny = df_yelp['label'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"1cb55f","input":"from sklearn.feature_extraction.text import CountVectorizer\n\nsentences = ['John likes ice cream', 'John hates chocolate.']\n\nvectorizer = CountVectorizer(min_df=0, lowercase=False)\nvectorizer.fit(sentences)\n\n# create a bag of words == vocabulary size \nvectorizer.vocabulary_","output":{"0":{"data":{"text/plain":"{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}"},"exec_count":3,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"630e3d","input":"vectorizer.transform(sentences).toarray()","output":{"0":{"data":{"text/plain":"array([[1, 0, 1, 0, 1, 1],\n       [1, 1, 0, 1, 0, 0]])"},"exec_count":4,"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"75dbd1","input":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"ca4ce0","input":"X_train","output":{"0":{"data":{"text/plain":"<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n\twith 7368 stored elements in Compressed Sparse Row format>"},"exec_count":7,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"9a00f4","input":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\n\nprint(\"Accuracy:\", score)","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy: 0.796\n"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"9ff6cc","input":"for source in df['source'].unique():\n    df_source = df[df['source'] == source]\n    sentences = df_source['sentence'].values\n    y = df_source['label'].values\n\n    sentences_train, sentences_test, y_train, y_test = train_test_split(\n        sentences, y, test_size=0.25, random_state=1000)\n\n    vectorizer = CountVectorizer()\n    vectorizer.fit(sentences_train)\n    X_train = vectorizer.transform(sentences_train)\n    X_test  = vectorizer.transform(sentences_test)\n\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    score = classifier.score(X_test, y_test)\n    print('Accuracy for {} data: {:.4f}'.format(source, score))","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy for yelp data: 0.7960\nAccuracy for amazon data: 0.7960\nAccuracy for imdb data: 0.7487\n"}},"pos":16,"type":"cell"}
{"cell_type":"markdown","id":"004ca9","input":"#### logistic regression ","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"0ea8c8","input":"Now, you can take each sentence and get the word occurrences of the words based on the previous vocabulary. ","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"4390f8","input":"You can see that the resulting feature vectors have 750 samples which are the number of training samples we have after the train-test split. Each sample has 2505 dimensions which is the size of the vocabulary. ","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"60df89","input":"You can see that this fairly simple model achieves a fairly good accuracy. It would be interesting to see whether we are able to outperform this model.","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"7343fc","input":"#### voctorization \nuse again on the previous BOW model to vectorize the sentences. ","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"801802","input":"#### split training and testing dataset\n\nWe start by taking the Yelp data as an example first. ","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"867c55","input":"#### Vectorization \n\nYou can use the CountVectorizer provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. ","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"8a3577","input":"You can see that the logistic regression reached an impressive 79.6%, but letâ€™s have a look how this model performs on the other data sets that we have. In this script, we perform and evaluate the whole process for each data set that we have:","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"ff845b","input":"#### load data \n\nhttps://archive-beta.ics.uci.edu/ml/datasets/331","pos":0,"type":"cell"}
{"id":0,"time":1627542257926,"type":"user"}
{"last_load":1627542258056,"type":"file"}